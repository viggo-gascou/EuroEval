# 游젏릖 Slovak

This is an overview of all the datasets used in the Slovak part of EuroEval. The
datasets are grouped by their task - see the [task overview](/tasks) for more
information about what these constitute.

## Sentiment Classification

### CSFD Sentiment-sk

This dataset was published in [this paper](https://aclanthology.org/R13-1016/) and
consists of reviews from the the Czech/Slovak Movie
Database (CSFD).

The original dataset contains 25,000 / 2,500 / 2,500 samples for the training,
validation, and test splits, respectively. We use 1,024 / 256 / 2,048 samples for
our training, validation and test splits, respectively. All the new splits are
subsets of the original splits.

Here are a few examples from the training split:

```json
{
    "text": "J칩 Steve Buacemi...jinak sra캜ka",
    "label": "negative"
}
```

```json
{
    "text": "Letny oddychovy comicsovy blockbuster. Po celkom fresh traileri a hlavne podla momentalnych hodnoteni (89%, 76. najlepsi film!!!) som cakal, ze to bude daka svieza pecka a prijemne prekvapenie. Ale nakoniec je to dost taky priemer. Taka ta klasika, universe s roznymi rasami, (anti)hrdinova, neoriginalna zapletka, kopa akcie.. Co vycnievalo boli zaujimave postavy a hlavne vtipne hlasky. Prave tymto sa mohol film viac odlisit od ostatnych comicsoviek, vtedy by som isiel s hodnotenim vyssie.. Od polky filmu mi bolo jasne, ze to je kvazi ochutnavka na (minimalne) dalsie 2 casti, tak uvidime kam to posunu. [#40/2013]",
    "label": "neutral"
}
```

```json
{
    "text": "Prevapivo pr칤jemn칠, vtipn칠, rozpr치vkov칠. Kone캜ne fantasy film, ktor칳 sa s칰stred칤 na rozpr치vanie pr칤behu a nepotrebuje k tomu zbesil칠 tempo ani ve쬶olep칠 po캜칤ta캜ov칠 arm치dy. Vo svojej podstate to nie je a tak칠 origin치lne, ale je tam p치r zauj칤mav칳ch n치padov a ako celok to skvelo funguje - v코etko je skr치tka na svojom mieste...",
    "label": "positive"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  Ni쮄멸e s칰 dokumenty a ich sentiment, ktor칳 m칪쬰 by콘 'pozit칤vne', 'neutr치lne' alebo 'negat칤vne'.
  ```

- Base prompt template:

  ```text
  Dokument: {text}
  Sentiment: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Dokument: {text}

  Klasifikujte pocit v dokumente. Odpovedzte so 'pozit칤vne', 'neutr치lne', alebo 'negat칤vne', a ni캜 in칠.
  ```

- Label mapping:
  - `positive` 俱뫮잺 `pozit칤vne`
  - `neutral` 俱뫮잺 `neutr치lne`
  - `negative` 俱뫮잺 `negat칤vne`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset csfd-sentiment-sk
```

## Named Entity Recognition

### UNER-sk

This dataset was published in
[this paper](https://aclanthology.org/2024.naacl-long.243/).

The original dataset consists of 8,482 / 1,059 / 1,060 samples for the
training, validation, and test splits, respectively. We use 1,024 / 256 / 2,048
samples for our training, validation and test splits, respectively. The train and
validation splits are subsets of the original splits, while the test split is
created using additional samples from the train split.

Here are a few examples from the training split:

```json
{
  "tokens": ["Bude", "ma콘", "n치zov", "Shanghai", "Noon", "a", "re쬴s칠rom", "bude", "debutuj칰ci", "Tom", "Dey", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-PER", "I-PER", "O"]
}
```

```json
{
  "tokens": ["Ako", "코es콘ro캜n칠ho", "(", "o", "rok", "sk칪r", ",", "ne", "bolo", "zvykom", ")", "ho", "na", "z치klade", "zvl치코tnej", "v칳nimky", "prijali", "medzi", "Zvedov", "a", "ako", "dev칛콘ro캜n칳", "sa", "stal", "ved칰cim", "skupiny", "."],
  "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-ORG", "O", "O", "O", "O", "O", "O", "O", "O"]
}
```

```json
{
  "tokens": ["To", "predsa", "stoj칤", "za", "pokus", "!"],
  "labels": ["O", "O", "O", "O", "O", "O"]
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 8
- Prefix prompt:

  ```text
  Nasleduj칰ce s칰 vety a JSON-objekty s pomenovan칳mi entitami, ktor칠 sa nach치dzaj칰 v danej vete.
  ```

- Base prompt template:

  ```text
  Veta: {text}
  Pomenovan칠 entity: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Veta: {text}

  Identifikujte pomenovan칠 entity vo vete. V칳stup by mal by콘 vo forme JSON-objektu s k쮂줷꼀i 'osoba', 'miesto', 'organiz치cia' a 'r칪zne'. Hodnoty by mali by콘 zoznamy pomenovan칳ch ent칤t danej kateg칩rie, presne tak, ako sa vyskytuj칰 vo vete.
  ```

- Label mapping:
  - `B-PER` 俱뫮잺 `osoba`
  - `I-PER` 俱뫮잺 `osoba`
  - `B-LOC` 俱뫮잺 `miesto`
  - `I-LOC` 俱뫮잺 `miesto`
  - `B-ORG` 俱뫮잺 `organiz치cia`
  - `I-ORG` 俱뫮잺 `organiz치cia`
  - `B-MISC` 俱뫮잺 `r칪zne`
  - `I-MISC` 俱뫮잺 `r칪zne`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset uner-sk
```

## Linguistic Acceptability

### ScaLA-sk

This dataset was published in [this paper](https://aclanthology.org/2023.nodalida-1.20/)
and was automatically created from the [Slovak Universal Dependencies
treebank](https://github.com/UniversalDependencies/UD_Slovak-SNK) by assuming that the
documents in the treebank are correct, and corrupting the samples to create
grammatically incorrect samples. The corruptions were done by either removing a word
from a sentence, or by swapping two neighbouring words in a sentence. To ensure that
this does indeed break the grammaticality of the sentence, a set of rules were used on
the part-of-speech tags of the words in the sentence.

The original full dataset consists of 1,024 / 256 / 2,048 samples for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
used as-is in the framework.

Here are a few examples from the training split:

```json
{
    "text": "Niektor칤 pozorovatelia pova쬿j칰 ropn칠 z치ujmy USA za jednu z hlavn칳ch motiv치ci칤 vstupu do vojny v Iraku.",
    "label": "correct"
}
```

```json
{
    "text": "Pop치li콘 sa na jedinom p칤smene je klasick칳 pr칤pad, ktor칳 sa m칪쬰 vyskytn칰콘 v r칪znych podob치ch.",
    "label": "correct"
}
```

```json
{
    "text": "Zo strachu o seba, pre svoju pov칳코en칰 zbabelos콘 zaprel svojho Majstra P치na.",
    "label": "incorrect"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 12
- Prefix prompt:

  ```text
  Nasleduj칰 vety a 캜i s칰 gramaticky spr치vne.
  ```

- Base prompt template:

  ```text
  Veta: {text}
  Gramaticky spr치vna: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Veta: {text}

  Ur캜ite, 캜i je veta gramaticky spr치vna alebo nie. Odpovedzte so '치no', ak je veta spr치vna, a 'nie', ak nie je. Odpovedzte iba t칳mto slovom, a ni캜 in칠.
  ```

- Label mapping:
  - `correct` 俱뫮잺 `치no`
  - `incorrect` 俱뫮잺 `nie`

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset scala-sk
```

## Reading Comprehension

### MultiWikiQA-sk

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2509.04111)
and contains Wikipedia articles with LLM-generated questions and answers in 300+
languages.

The original full dataset consists of 5,000 samples in a single split. We use a 1,024 /
256 / 2,048 split for training, validation and testing, respectively, sampled randomly.

Here are a few examples from the training split:

```json
{
  "context": "Register toxick칳ch 칰캜inkov chemick칳ch l치tok (anglicky Registry of Toxic Effects of Chemical Substances, RTECS) je datab치za toxikologick칳ch inform치ci칤 zostaven칳ch z vo쬹e dostupnej vedeckej literat칰ry bez odkazu na platnos콘 alebo u쬴to캜nos콘 publikovan칳ch 코t칰di칤. Do roku 2001 bola datab치za spravovan치 americkou organiz치ciou NIOSH (National Institute for Occupational Safety and Health, slov. N치rodn칳 칰stav pre bezpe캜nos콘 a ochranu zdravia pri pr치ci) ako verejne dostupn치 publik치cia. Teraz ju spravuje s칰kromn치 spolo캜nos콘 Symyx Technologies a je dostupn치 len za poplatok.\n\nObsah \nDatab치za obsahuje 코es콘 typov toxikologick칳ch inform치ci칤:\n prim치rne podr치쬯enie\n mutag칠nne 칰캜inky\n reproduk캜n칠 칰캜inky\n karcinog칠nne 칰캜inky\n ak칰tna toxicita\n toxicita viacn치sobn칳ch d치vok\nV datab치ze sa spom칤naj칰 ako 코pecifick칠 캜칤seln칠 hodnoty, ako napr칤klad LD50, LC50, TDLo alebo TCLo, tak aj 코tudovan칠 organizmy a sp칪sob pod치vania l치tky. Pre v코etky d치ta s칰 uveden칠 bibliografick칠 zdroje. 맚칰die pritom nie s칰 nijako hodnoten칠.\n\nHist칩ria \nDatab치za RTECS bola aktivitou schv치lenou americk칳m Kongresom, zakotvenou v Sekcii 20(a)(6) z치kona Occupational Safety and Health Act z roku 1970 (PL 91-596). P칪vodn칠 vydanie, zn치me ako Zoznam toxick칳ch l치tok (Toxic Substances List), bolo publikovan칠 28. j칰na 1971 a obsahovalo toxikologick칠 d치ta o pribli쬹e 5 000 chemik치li치ch. N치zov bol nesk칪r zmenen칳 na dne코n칳 Register toxick칳ch 칰캜inkov chemick칳ch l치tok (Registry of Toxic Effects of Chemical Substances). V janu치ri 2001 datab치za obsahovala 152 970 chemik치li칤. V decembri 2001 bola spr치va RTECS preveden치 z NIOSH do s칰kromnej firmy Elsevier MDL. T칰to firmu k칰pila v roku 2007 spolo캜nos콘 Symyx, s칰캜as콘ou akviz칤cie bola aj datab치za RTECS. T치 je teraz dostupn치 len za poplatok vo forme ro캜n칠ho predplatn칠ho.\n\nRTECS je k dispoz칤cii v angli캜tine, franc칰z코tine a 코paniel캜ine, a to prostredn칤ctvom Kanadsk칠ho centra pre bezpe캜nos콘 a ochranu zdravia pri pr치ci. Predplatitelia maj칰 pr칤stup cez web, na CD-ROM a vo form치te pre intranet. Datab치za je dostupn치 na webe aj cez NISC (National Information Services Corporation) a ExPub (Expert Publishing, LLC).\n\nExtern칠 odkazy \n\n RTECS overview \n Symyx website \n Expert Publishing, LLC Website\n\nZdroj \n\nChemick칠 n치zvy a k칩dy\nToxikol칩gia",
  "question": "Ak칠 s칰 tri mo쬹osti pr칤stupu k datab치ze RTECS, ak som predplatite?",
  "answers": {"answer_start": [1949], "text": ["cez web, na CD-ROM a vo form치te pre intranet"]}}
```

```json
{
  "context": "Herta Naglov치-Docekalov치 (* 29. m치j 1944, Wels, Rak칰sko) je rak칰ska filozofka a profesorka, 캜lenka vedenia Medzin치rodnej asoci치cie filozofiek (IAPf), 칐sterreichische Akademie der Wissenschaften, Institut International de Philosophie (Par칤), viceprezidentka F칠d칠ration Internationale des Soci칠t칠s de Philosophie (FISP), zakladaj칰ca 캜lenka interdisciplin치rnych pracovn칳ch skup칤n Frauengeschichte a Philosophische Frauenforschung na Viedenskej univerzite, 캜lenka redak캜n칳ch r치d popredn칳ch vedeck칳ch 캜asopisov, napr. Philosophin, L췂Homme, Deutsche Zeitschrift f칲r Philosophie.\n\n콯ivotopis \nVy코tudovala hist칩riu, filozofiu a germanistiku na Viedenskej univerzite. V roku 1967 z칤skala na svojej alma mater doktor치t z hist칩rie pr치cou o filozofovi dej칤n Ernstovi von Lasaulx). V rokoch 1968 - 1985 bola asistentkou na In코tit칰te filozofie Viedenskej univerzity. V lete 1980 predn치코ala na Millersville University of Pennsylvania v USA.\n\nV roku 1981 sa habilitovala z filozofie na Viedenskej univerzite dielom Die Objektivit칛t der Geschichtswissenschaft. V rokoch 1985 a 2009 bola profesorkou In코tit칰tu filozofie Viedenskej univerzity. Od roku 2009 je univerzitnou profesorkou na d칪chodku (Universit칛tsprofessorin i. R.)\n\nBola hos콘uj칰cou profesorkou v roku 1990 na Universiteit Utrecht v holandskom Utrechte; v Nemecku 1991/1992 na Goethe-Universit칛t Frankfurt vo Frankfurte nad Mohanom; 1993 na Universit칛t Konstanz v Konstanzi; 1994/1995 na Freie Universit칛t Berlin v Berl칤ne. V rokoch 1995/1996 predn치코ala na Universit칛t Innsbruck a 2011 na univerzite v Petrohrade v Rusku.\n\nDielo (v칳ber) \n Jenseits der S칛kularisierung. Religionsphilosophische Studien. - Berlin 2008 (Hg., gem.m. Friedrich Wolfram).\n Viele Religionen - eine Vernunft? Ein Disput zu Hegel. - Wien/Berlin 2008 (Hg., gem.m. Wolfgang Kaltenbacher und Ludwig Nagl).\n Glauben und Wissen. Ein Symposium mit J칲rgen Habermas. - Wien/Berlin 2007 (Hg., gem.m. Rudolf Langthaler).\n Geschichtsphilosophie und Kulturkritik. - Darmstadt 2003 (Hrsg., gem.m. Johannes Rohbeck).\n Feministische Philosophie. Ergebnisse, Probleme, Perspektiven. - Frankfurt a.M. 2000 a 2004 \n Continental Philosophy in Feminist Perspective. - Pennsylviania State University Press 2000 (Hg. gem.m. Cornelia Klingler).\n Der Sinn des Historischen. - Frankfurt a.M. 1996 (Hrsg.).\n Politische Theorie. Differenz und Lebensqualit칛t. - Frankfurt a.M. 1996 (Hrsg. gem.m. Herlinde Pauer-Studer).\n Postkoloniales Philosophieren: Afrika. - Wien/M칲nchen 1992 (Hrsg., gem.m. Franz Wimmer).\n Tod des Subjekts? - Wien/M칲nchen 1987 (Hrsg., gem.m. Helmuth Vetter).\n Die Objektivit칛t der Geschichtswissenschaft. Systematische Untersuchungen zum wissenschaftlichen Status der Historie. - Wien/M칲nchen 1982\n spoluvydavate쬶a: Wiener Reihe. Themen der Philosophie (od 1986). \n spoluvydavate쬶a: Deutsche Zeitschrift f칲r Philosophie (1993-2004). \n spoluvydavate쬶a: L'Homme. Europ칛ische Zeitschrift f칲r feministische Geschichtswissenschaft (1990 - 2003).\n\nOcenenia \n F칬rderpreis mesta Viede켿, 1983\n K칛the Leichter Preis (rak칰ska 코t치tna cena), 1997 \n Preis f칲r Geistes- und Sozialwissenschaften der Stadt Wien, 2009\n\nReferencie\n\nExtern칠 odkazy \n Ofici치lna str치nka, Universit칛t Wien \n Austria Forum, Wissenssammlungen/Biographien: Herta Nagl-Docekal\n\nZdroj \n\nRak칰ski filozofi",
  "question": "Kedy pri코la na svet Herta Naglov치-Docekalov치?",
  "answers": {"answer_start": [28], "text": ["29. m치j 1944"]}}
```

```json
{
  "context": "Martin Bare코 (* 25. november 1968, Brno) je 캜esk칳 profesor neurol칩gie, od septembra 2019 rektor Masarykovej univerzity, predt칳m od febru치ra 2018 do septembra 2019 dekan Lek치rskej fakulty Masarykovej univerzity.\n\nRiadiace funkcie \nVo febru치ri 2018 sa stal dekanom Lek치rskej fakulty Masarykovej univerzity. Funkciu prevzal po Ji콏칤m Mayerovi, ktor칳 zast치val poz칤ciu dekana v obdob칤 20102018. S n치stupom na post dekana ukon캜il svoje p칪sobenie ako prorektor univerzity, ako i z치stupca prednostu I. neurologickej kliniky pre vedu a v칳skum.\n\nDo funkcie rektora univerzity bol zvolen칳 1. apr칤la 2019 Akademick칳m sen치tom Masarykovej univerzity. V prvom kole tajnej vo쬭y z칤skal Bare코 36 hlasov z 50 pr칤tomn칳ch sen치torov. Protikandid치ta, prodekana Pr칤rodovedeckej fakulty Jarom칤ra Leichmana, volilo 11 sen치torov. 3 odovzdan칠 hlasy boli neplatn칠.\n\nSk칰senosti s p칪soben칤m vo veden칤 코koly zbieral Bare코 v rokoch 20112018, kedy p칪sobil najsk칪r ako jej prorektor pre rozvoj a potom ako prorektor pre akademick칠 z치le쬴tosti. Za svoje priority ozna캜il Bare코 v dobe vo쬭y posil켿ovanie role univerzity ako piliera slobody v s칰캜asnej spolo캜nosti a zv칳코enie kvality vzdel치vania, vedy a v칳skumu na medzin치rodnej 칰rovni.\n\nDo funkcie rektora ho vymenoval 11. j칰na 2019 prezident Milo코 Zeman s 칰캜innos콘ou od 1. septembra 2019. Vo funkcii tak nahradil Mikul치코a Beka, ktor칠mu sa skon캜ilo druh칠 volebn칠 obdobie a o zvolenie sa teda u op칛콘 uch치dza콘 nemohol. Bare코 k 1. septembru 2019 rezignoval na post dekana Lek치rskej fakulty.\n\nVedeck치 캜innos콘 \nJe predn치코aj칰cim v odboroch v코eobecn칠 lek치rstvo, zubn칠 lek치rstvo, optometria, fyzioterapia, neurofyziol칩gia pre 코tudentov pr칤rodn칳ch vied Lek치rskej fakulty Masarykovej univerzity a 코kolite doktorandov odborovej rady neurol칩gia a neurovedy.\n\nP칪sob칤 v t칳chto vedeck칳ch rad치ch: Masarykova univerzita, Lek치rska fakulta Masarykovej univerzity a CEITEC MU. 캝alej tie Univerzita Palack칠ho v Olomouci, Lek치rska fakulta UPOL, Fakulta veterin치rn칤ho l칠ka콏stv칤 VFU, 캞alej je tie 캜lenom 캛eskej lek치rskej komory, 캛eskej neurologickej spolo캜nosti, 캛eskej spolo캜nosti klinickej neurofyziol칩gie, 캛eskej lek치rskej spolo캜nosti Jana Evangelisty Purkyn캩, Movement Disorders Society, Society for the Research on the Cerebellum a Society for Neuroscience. Takisto je 캜lenom redak캜nej rady 캜asopisov Clinical Neurophysiology, Behavioural Neurology, Tremor and Other Hyperkinetic Movements a Biomedical Papers.\n\nOsobn칳 쬴vot \nJe 쬰nat칳, m치 dvoch synov a dc칠ru.\n\nReferencie\n\nExtern칠 odkazy \n Martin Bare코\n\nZdroj \n\n캛esk칤 lek치ri\nNeurol칩govia\nRektori Masarykovej univerzity\n캛esk칤 univerzitn칤 profesori\nDekani Lek치rskej fakulty Masarykovej univerzity\nAbsolventi Lek치rskej fakulty Masarykovej univerzity\nOsobnosti z Brna",
  "question": "Ak칰 poz칤ciu mal Martin Bare코 na Masarykovej univerzite po캜n칰c septembrom 2019?",
  "answers": {
    "answer_start": [89],
    "text": ["rektor"]
  }
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 4
- Prefix prompt:

  ```text
  Nasleduj칰 texty s pridru쬰n칳mi ot치zkami a odpove캞ami.
  ```

- Base prompt template:

  ```text
  Text: {text}
  Ot치zka: {question}
  Odpove캞 na maxim치lne 3 slov치:
  ```

- Instruction-tuned prompt template:

  ```text
  Text: {text}

  Odpovedzte na nasleduj칰cu ot치zku t칳kaj칰cu sa textu uveden칠ho vy코코ie maxim치lne 3 slovami.

  Ot치zka: {question}
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset multi-wiki-qa-sk
```

## Knowledge

### MMLU-sk

This dataset is a machine translated version of the English [MMLU
dataset](https://openreview.net/forum?id=d7KBjmI3GmQ) and features questions within 57
different topics, such as elementary mathematics, US history and law. The translation to
Slovak was done by the University of Oregon as part of [this
paper](https://aclanthology.org/2023.emnlp-demo.28/), using GPT-3.5-turbo.

The original full dataset consists of 269 / 1,410 / 13,200 samples for training,
validation and testing, respectively. We use a 1,024 / 256 / 2,048 split for training,
validation and testing, respectively (so 3,328 samples used in total). These splits are
new and there can thus be some overlap between the original validation and test sets and
our validation and test sets.

Here are a few examples from the training split:

```json
{
  "text": "V ak칳ch smeroch je pr칤pad pre humanit치rnu intervenciu, ako je uveden칠 v tejto kapitol... mocn칳mi 코t치tmi.\nd. V코etky tieto mo쬹osti.",
  "label": "d",
}
```

```json
{
  "text": "FAKTORI츼LOV칗 ANOVA sa pou쮂셨a v pr칤pade, 쬰 코t칰dia zah콋켿a viac ako 1 VI. Ak칳 je INTER...캜inok VI na rovnakej 칰rovni ako ostatn칠 VI",
  "label": "a"
}
```

```json
{
  "text": "Pre ktor칰 z t칳chto dvoch situ치ci칤 urob칤 hlavn치 postava (ktor치 pou쮂셨a ja/m켿a/m칪j) nie...ie zl칠\nc. Nie zl칠, zl칠\nd. Nie zl칠, nie zl칠",
  "label": "d",
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj칰 ot치zky s viacer칳mi mo쬹os콘ami (s odpove캞ami).
  ```

- Base prompt template:

  ```text
  Ot치zka: {text}
  Mo쬹osti:
  a. {option_a}
  b. {option_b}
  c. {option_c}
  d. {option_d}
  Odpove캞: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot치zka: {text}

  Odpovedzte na nasleduj칰cu ot치zku pou쬴t칤m 'a', 'b', 'c' alebo 'd', a ni캜 in칠.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset mmlu-sk
```

## Common-sense Reasoning

### Winogrande-sk

This dataset was published in [this paper](https://doi.org/10.48550/arXiv.2506.19468)
and is a translated and filtered version of the English [Winogrande
dataset](https://doi.org/10.1145/3474381).

The original full dataset consists of 47 / 1,210 samples for training and testing, and
we use 128 of the test samples for validation, resulting in a 47 / 128 / 1,085 split for
training, validation and testing, respectively.

Here are a few examples from the training split:

```json
{
  "text": "Nedok치zal som ovl치da콘 vlhkos콘 ako som ovl치dal d치쮃, preto쬰 _ prich치dzalo odv코adia. Na koho sa vz콘ahuje pr치zdne miesto _?\nMo쬹osti:\na. vlhkos콘\nb. d치쮃",
  "label": "a"
}
```

```json
{
  "text": "Jessica si myslela, 쬰 Sandstorm je najlep코ia piese켿, ak치 bola kedy nap칤san치, ale Patricia ju nen치videla. _ si k칰pila l칤stok na jazzov칳 koncert. Na koho sa vz콘ahuje pr치zdne miesto _?\nMo쬹osti:\na. Jessica\nb. Patricia",
  "label": "b"
}
```

```json
{
  "text": "Termostat ukazoval, 쬰 dole bolo o dvadsa콘 stup켿ov chladnej코ie ako hore, tak쬰 Byron zostal v _ preto쬰 mu bola zima. Na koho sa vz콘ahuje pr치zdne miesto _?\nMo쬹osti:\na. dole\nb. hore",
  "label": "b"
}
```

When evaluating generative models, we use the following setup (see the
[methodology](/methodology) for more information on how these are used):

- Number of few-shot examples: 5
- Prefix prompt:

  ```text
  Nasleduj칰 ot치zky s viacer칳mi mo쬹os콘ami (s odpove캞ami).
  ```

- Base prompt template:

  ```text
  Ot치zka: {text}
  Mo쬹osti:
  a. {option_a}
  b. {option_b}
  Odpove캞: {label}
  ```

- Instruction-tuned prompt template:

  ```text
  Ot치zka: {text}
  Mo쬹osti:
  a. {option_a}
  b. {option_b}

  Odpovedzte na nasleduj칰cu ot치zku pou쬴t칤m 'a' alebo 'b', a ni캜 in칠.
  ```

You can evaluate this dataset directly as follows:

```bash
euroeval --model <model-id> --dataset winogrande-sk
```
